<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>最小生成树</title>
    <link href="/posts/57313.html"/>
    <url>/posts/57313.html</url>
    
    <content type="html"><![CDATA[<hr><div class="note note-success">            <p>数学建模 图论 Python</p>          </div><h3 id="最小生成树算法及其networkx实现">最小生成树算法及其networkx实现</h3><p>树是图论中非常重要的一类图，它非常类似于自然界的树，结构简单、应用广泛，最小生成树问题则是其中经典的问题之一。在实际应用中，许多问题的图论模型都是最小生成树，如通信网络建设、有线电缆铺设、加工设备分组等。关键词：连通，无圈，最短。</p><h3 id="基本概念">基本概念</h3><p>定义：连通的无圈图称为树。</p><p>定理：设<span class="math inline">\(G\)</span>是具有<span class="math inline">\(n\)</span>个顶点<span class="math inline">\(m\)</span>条边的图，则下列命题等价：</p><p>（1）图<span class="math inline">\(G\)</span>是树；</p><p>（2）图<span class="math inline">\(G\)</span>中任意两个不同顶点之间存在唯一的路；</p><p>（3）图<span class="math inline">\(G\)</span>连通，删除任一条边均不连通；</p><p>（4）图<span class="math inline">\(G\)</span>连通，且<span class="math inline">\(n=m+1\)</span>；</p><p>（5）图<span class="math inline">\(G\)</span>无圈，添加任一条边可得唯一的圈；</p><p>（6）图<span class="math inline">\(G\)</span>无圈，且<span class="math inline">\(n=m+1\)</span>.</p><p>定义：若图<span class="math inline">\(G\)</span>的生成子图<span class="math inline">\(H\)</span>是树，则称<span class="math inline">\(H\)</span>为<span class="math inline">\(G\)</span>的生成树。一个图的生成树往往不唯一。</p><p>定理：连通图的生成树一定存在。</p><p>定义：在赋权图<span class="math inline">\(G\)</span>中，边权之和最小的生成树称为<span class="math inline">\(G\)</span>的最小生成树。</p><h3 id="最小生成树算法">最小生成树算法</h3><p>构造连通图最小生成树的算法有Kruskal算法和Prim算法。均可以在networkx中实现。</p><p>例：已知8口井，相互之间的距离如下表，已知1号油井离海岸线最近，为5nmil。问从海岸经一号油井铺设管道将各油井连接起来，应如何铺设使油管长度最短。</p><p><img src="https://drive.imgod.me/api/v3/file/get/61997/QQ%E6%88%AA%E5%9B%BE20220627191701.png?sign=NwhSbk0-PlOFJlM5PfxCTKCgD-TQs53UdwnVkUffd4A%3D%3A0"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> networkx <span class="hljs-keyword">as</span> nx<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>sns.set_theme(style=<span class="hljs-string">&quot;dark&quot;</span>)<span class="hljs-comment">#用来设置主题背景</span><br>plt.rcParams[<span class="hljs-string">&quot;font.sans-serif&quot;</span>] = [<span class="hljs-string">&quot;SimHei&quot;</span>] <span class="hljs-comment">#用来正常显示中文标签</span><br>plt.rcParams[<span class="hljs-string">&quot;axes.unicode_minus&quot;</span>] = <span class="hljs-literal">False</span> <span class="hljs-comment">#用来正常显示负号</span><br>W_input = np.array([[<span class="hljs-number">0</span>,<span class="hljs-number">1.3</span>,<span class="hljs-number">2.1</span>,<span class="hljs-number">0.9</span>,<span class="hljs-number">0.7</span>,<span class="hljs-number">1.8</span>,<span class="hljs-number">2.0</span>,<span class="hljs-number">1.5</span>]<br>            ,[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0.9</span>,<span class="hljs-number">1.8</span>,<span class="hljs-number">1.2</span>,<span class="hljs-number">2.6</span>,<span class="hljs-number">2.3</span>,<span class="hljs-number">1.1</span>]<br>            ,[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">2.6</span>,<span class="hljs-number">1.7</span>,<span class="hljs-number">2.5</span>,<span class="hljs-number">1.9</span>,<span class="hljs-number">1.0</span>]<br>            ,[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0.7</span>,<span class="hljs-number">1.6</span>,<span class="hljs-number">1.5</span>,<span class="hljs-number">0.9</span>]<br>            ,[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0.9</span>,<span class="hljs-number">1.1</span>,<span class="hljs-number">0.8</span>]<br>            ,[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0.6</span>,<span class="hljs-number">1.0</span>]<br>            ,[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0.5</span>]<br>             ,[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]])<br>W = W_input.T+W_input <span class="hljs-comment">#生成邻接矩阵</span><br>G  = nx.Graph(W) <span class="hljs-comment">#创建无向图</span><br>T = nx.minimum_spanning_tree(G) <span class="hljs-comment">#根据图G生成最小生成树</span><br>D = nx.to_numpy_matrix(T) <span class="hljs-comment">#获取最小生成树的邻接矩阵</span><br>plt.figure(dpi=<span class="hljs-number">500</span>)<br>node_labels = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">8</span>),<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">9</span>))) <span class="hljs-comment">#顶点标签</span><br>pos = nx.shell_layout(T)<span class="hljs-comment">#设置布局</span><br>w = nx.get_edge_attributes(T,<span class="hljs-string">&quot;weight&quot;</span>) <span class="hljs-comment">#获取边的权重</span><br>nx.draw_networkx(T,pos,node_size=<span class="hljs-number">270</span>,labels=node_labels) <span class="hljs-comment">#绘制图</span><br>nx.draw_networkx_edge_labels(T,pos,font_size=<span class="hljs-number">10</span>,edge_labels=w)<span class="hljs-comment">#标记权重</span><br>plt.savefig(<span class="hljs-string">&quot;figure4.png&quot;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>结果如下：</p><p><img src="https://drive.imgod.me/api/v3/file/get/61998/figure4.png?sign=pznKdZ39N6Ar0fpWKSJM5NXvt2t6ZvjY-SviJfb3S4c%3D%3A0"></p>]]></content>
    
    
    <categories>
      
      <category>数学建模</category>
      
      <category>图论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学建模</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>最短路径及其应用</title>
    <link href="/posts/24890.html"/>
    <url>/posts/24890.html</url>
    
    <content type="html"><![CDATA[<hr><div class="note note-success">            <p>数学建模 图论 Python</p>          </div><h2 id="最短路算法及其python实现">最短路算法及其Python实现</h2><p>最短路径问题是图论中非常经典的问题之一，旨在寻找图中两顶点之间的最短路径。作为一个基本工具，实际应用中许多优化问题，如管道铺设、线路安排、工区布局、设备更新等。都可被归结为最短路径问题来解决。</p><p>定义1：设图<span class="math inline">\(G\)</span>是赋权图，<span class="math inline">\(\Gamma\)</span>为<span class="math inline">\(G\)</span>中的一条路，则称<span class="math inline">\(\Gamma\)</span>的各边权之和为路<span class="math inline">\(\Gamma\)</span>的长度。</p><p>对于<span class="math inline">\(G\)</span>的两个顶点<span class="math inline">\(u_0\)</span>和<span class="math inline">\(v_0\)</span>，从<span class="math inline">\(u_0\)</span>到<span class="math inline">\(v_0\)</span>的路不止一条，其中最短的一条称为从<span class="math inline">\(u_0\)</span>到<span class="math inline">\(v_0\)</span>的最短路；最短路的长称为从<span class="math inline">\(u_0\)</span>到<span class="math inline">\(v_0\)</span>的距离，记为<span class="math inline">\(d(u_0,v_0)\)</span>。</p><p>求最短路的算法有Dijkstra标号算法和Floyd算法，但Dijkstra标号算法只适应于边权为非负的情形，最短路径的问题也可以归结到<span class="math inline">\(0-1\)</span>规划问题。</p><h3 id="设备更新问题">设备更新问题</h3><p>某种工程设备的役龄为4年，每年年初都面临着是否更新的问题；若卖旧买新，就需要支付一定的购置费用；若继续使用，则要支付更多的维护费用，且使用年限越长维护费用就越多。若役龄期间每年的年初购置价格，当年维护费用及其年末剩余净值如下表所示。请为该设备制定一个4年役龄期内的更新计划，使总的支付费用最少。</p><table><thead><tr class="header"><th style="text-align: center;">年份</th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">年初购置价格/万元</td><td>25</td><td>26</td><td>28</td><td>31</td></tr><tr class="even"><td style="text-align: center;">当年维护费用/万元</td><td>10</td><td>14</td><td>18</td><td>26</td></tr><tr class="odd"><td style="text-align: center;">年末剩余净值/万元</td><td>20</td><td>16</td><td>13</td><td>11</td></tr></tbody></table><p>解： 可以把该问题进行数学抽象</p><p>构造赋权有向图<span class="math inline">\(D=(V,A,W)\)</span>，其中顶点集<span class="math inline">\(V=\{v_1,v_2,v_3,v_4,v_5\},where\quadv_i(i=1,2,3,4)\text{表示第i年初},v_5\text{表示第5年初(第4年末)}.\)</span>,<span class="math inline">\(A\)</span>表示弧集，邻接矩阵<span class="math inline">\(W=(w_{ij})_{5\times5}\)</span>，这里<span class="math inline">\(w_{ij}\)</span>表示第<span class="math inline">\(i\)</span>年初购进机器第<span class="math inline">\(j\)</span>年初卖掉这个过程一共支付的费用，计算公式为<span class="math display">\[w_{ij}=p_i+\sum_{k=1}^{j-i}a_k-r_{j-i}\]</span> 其中<span class="math inline">\(p_i\)</span>为第<span class="math inline">\(i\)</span>年年初的购置价格,<span class="math inline">\(a_k\)</span>表示从购入开始计算使用到第k年的费用，<span class="math inline">\(r_i\)</span>表示使用<span class="math inline">\(i\)</span>年旧设备的出售价格。则邻接矩阵为： <span class="math display">\[W = \begin{bmatrix}            0&amp;15&amp;33&amp;54&amp;82\\            \infty&amp;0&amp;16&amp;34&amp;55\\            \infty&amp;\infty&amp;0&amp;18&amp;36\\            \infty&amp;\infty&amp;\infty&amp;0&amp;21\\            \infty&amp;\infty&amp;\infty&amp;\infty&amp;0            \end{bmatrix}\nonumber\]</span> 经计算得：</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs clean">最短路径为：<br><span class="hljs-number">1</span>-&gt;<span class="hljs-number">2</span>-&gt;<span class="hljs-number">3</span>-&gt;<span class="hljs-number">5</span><br>最短距离为<span class="hljs-number">67</span><br></code></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/26/l4HTiCy5cR62zW8.png" style="zoom: 50%;"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python">W = np.array([[<span class="hljs-number">0</span>,<span class="hljs-number">15</span>,<span class="hljs-number">33</span>,<span class="hljs-number">54</span>,<span class="hljs-number">82</span>]<br>              ,[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">16</span>,<span class="hljs-number">34</span>,<span class="hljs-number">55</span>]<br>              ,[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span> ,<span class="hljs-number">18</span>,<span class="hljs-number">36</span>]<br>              ,[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span> ,<span class="hljs-number">0</span> , <span class="hljs-number">21</span> ]<br>              ,[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span> ,<span class="hljs-number">0</span> , <span class="hljs-number">0</span> ]])<br><span class="hljs-string">&#x27;&#x27;&#x27;注意：在利用networkx库函数计算时，如果两个顶点之间没有边，对应的邻接矩阵元素为0，而不是np.inf&#x27;&#x27;&#x27;</span><br>G=nx.DiGraph(W)<span class="hljs-comment">#创建有向图</span><br>p = nx.dijkstra_path(G,source=<span class="hljs-number">0</span>,target=<span class="hljs-number">4</span>,weight=<span class="hljs-string">&quot;weight&quot;</span>)<span class="hljs-comment">#求最短路径</span><br><span class="hljs-string">&quot;&quot;&quot;param G:图</span><br><span class="hljs-string">   param source:起点</span><br><span class="hljs-string">   param target:终点</span><br><span class="hljs-string">   param weight:用边权作为路长&quot;&quot;&quot;</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;最短路径为：&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;-&gt;&quot;</span>.join([<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> p]))<br>d = nx.dijkstra_path_length(G,source=<span class="hljs-number">0</span>,target=<span class="hljs-number">4</span>,weight=<span class="hljs-string">&quot;weight&quot;</span>)<span class="hljs-comment">#求最短距离</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;最短距离为&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(d))<br>plt.figure(dpi=<span class="hljs-number">500</span>)<br>node_labels=<span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>),[<span class="hljs-string">&quot;V&quot;</span>+<span class="hljs-built_in">str</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">6</span>)])) <span class="hljs-comment">#构造用于顶点标记的字典</span><br>pos = nx.shell_layout(G)<span class="hljs-comment">#设置布局</span><br>w = nx.get_edge_attributes(G,<span class="hljs-string">&quot;weight&quot;</span>) <span class="hljs-comment">#获取边的权重</span><br>nx.draw_networkx(G,pos,node_size=<span class="hljs-number">270</span>,labels=node_labels) <span class="hljs-comment">#绘制图</span><br>nx.draw_networkx_edge_labels(G,pos,font_size=<span class="hljs-number">10</span>,edge_labels=w)<span class="hljs-comment">#标记权重</span><br>path_edges=<span class="hljs-built_in">list</span>(<span class="hljs-built_in">zip</span>(p,p[<span class="hljs-number">1</span>:]))<span class="hljs-comment">#标记路径的元组组成的列表</span><br>nx.draw_networkx_edges(G,pos,edgelist=path_edges,edge_color=<span class="hljs-string">&quot;r&quot;</span>,width=<span class="hljs-number">3</span>)<span class="hljs-comment">#标记路径</span><br>plt.savefig(<span class="hljs-string">&quot;figure2.png&quot;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><h3 id="重心问题">重心问题</h3><p>有些公共服务设施（例如邮局，学校等）的选址，要求设施到所有服务对象点的距离总和最小。一般要考虑人口密度问题，或者全体被服务对象来往的总路程最短。</p><p>某矿区有六个矿点，如下图所示，已知各产矿点每天的产煤量为<span class="math inline">\(q_i(i=1,2,3,4,5,6)t,\)</span>,现在要从这六个矿点选一个作为矿场。问在那个产矿点，才能使各矿点所产的矿石运到选矿厂所在地的总运力<span class="math inline">\((t\cdot km)\)</span>最小。</p><p><img src="https://drive.imgod.me/api/v3/file/get/61808/figure3.png?sign=4h_GOqbwtj4VVuzNQR-NVKtZ2LoCeR7rOVKRgHyJZds%3D%3A0" style="zoom:50%;"></p><p>解：令<span class="math inline">\(d_{ij}(i,j=1,2,\cdots,6)\)</span>表示顶点<span class="math inline">\(v_i\)</span>和顶点<span class="math inline">\(v_j\)</span>之间的距离。若选矿厂设在<span class="math inline">\(m_i\)</span>，并且各产矿点到选矿厂的总运力为<span class="math inline">\(m_i\)</span>,则确定选矿厂的位置就转化为求<span class="math inline">\(m_k\)</span>，使得 <span class="math display">\[m_k = \min_{1\le i \le6} m_i\]</span>由于各产矿点到选矿厂的总运力依赖于任意两顶点之间的距离，即任意两顶点之间的最短距离，先用Floyd计算出所有顶点对之间的最短距离，然后计算出顶点<span class="math inline">\(v_i\)</span>作为设立选矿厂时各自到<span class="math inline">\(v_i\)</span>的总运力 <span class="math display">\[m_i = \sum_{j=1}^{6}q_jd_{ij},\quad i = 1,2,\cdots,6\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python">W = np.array([[<span class="hljs-number">0</span>,<span class="hljs-number">20</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">15</span>,<span class="hljs-number">0</span>]<br>            ,[<span class="hljs-number">20</span>,<span class="hljs-number">0</span>,<span class="hljs-number">20</span>,<span class="hljs-number">40</span>,<span class="hljs-number">25</span>,<span class="hljs-number">0</span>]<br>            ,[<span class="hljs-number">0</span>,<span class="hljs-number">20</span>,<span class="hljs-number">0</span>,<span class="hljs-number">30</span>,<span class="hljs-number">10</span>,<span class="hljs-number">0</span>]<br>            ,[<span class="hljs-number">0</span>,<span class="hljs-number">40</span>,<span class="hljs-number">30</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]<br>            ,[<span class="hljs-number">15</span>,<span class="hljs-number">25</span>,<span class="hljs-number">10</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">15</span>]<br>            ,[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">15</span>,<span class="hljs-number">0</span>]])<br><span class="hljs-comment">#绘制无向加权图</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> networkx <span class="hljs-keyword">as</span> nx<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br>sns.set_theme(style=<span class="hljs-string">&quot;dark&quot;</span>)<span class="hljs-comment">#用来设置主题背景</span><br>plt.rcParams[<span class="hljs-string">&quot;font.sans-serif&quot;</span>] = [<span class="hljs-string">&quot;SimHei&quot;</span>] <span class="hljs-comment">#用来正常显示中文标签</span><br>plt.rcParams[<span class="hljs-string">&quot;axes.unicode_minus&quot;</span>] = <span class="hljs-literal">False</span> <span class="hljs-comment">#用来正常显示负号</span><br>G = nx.Graph(W)<span class="hljs-comment">#创建无向图类</span><br>plt.figure(dpi=<span class="hljs-number">500</span>)<br>node_labels=<span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">6</span>),[<span class="hljs-string">&quot;v1(80)&quot;</span>,<span class="hljs-string">&quot;v2(90)&quot;</span>,<span class="hljs-string">&quot;v3(30)&quot;</span>,<span class="hljs-string">&quot;v4(20)&quot;</span>,<span class="hljs-string">&quot;v5(60)&quot;</span>,<span class="hljs-string">&quot;v6(10)&quot;</span>])) <span class="hljs-comment">#构造用于顶点标记的字典</span><br>pos = nx.shell_layout(G)<span class="hljs-comment">#设置布局</span><br>w = nx.get_edge_attributes(G,<span class="hljs-string">&quot;weight&quot;</span>) <span class="hljs-comment">#获取边的权重</span><br>nx.draw_networkx(G,pos,node_size=<span class="hljs-number">270</span>,labels=node_labels,font_size=<span class="hljs-number">5</span>) <span class="hljs-comment">#绘制图</span><br>nx.draw_networkx_edge_labels(G,pos,font_size=<span class="hljs-number">10</span>,edge_labels=w)<span class="hljs-comment">#标记权重</span><br>plt.savefig(<span class="hljs-string">&quot;figure3.png&quot;</span>)<br>plt.show()<br>d = nx.shortest_path_length(G,weight=<span class="hljs-string">&quot;weight&quot;</span>) <span class="hljs-comment">#计算各个点之间的最短路径</span><br>s = <span class="hljs-built_in">dict</span>(d) <span class="hljs-comment">#将其转化为字典</span><br>res = np.zeros((<span class="hljs-number">6</span>,<span class="hljs-number">6</span>))<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">6</span>):<br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">6</span>):<br>        res[i][j] = s[i][j]<span class="hljs-comment">#构建最短路径矩阵</span><br>df = pd.DataFrame(res,columns=[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>],index=[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>])<br>df.head(<span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure><p>结果如下：</p><p><img src="https://drive.imgod.me/api/v3/file/get/61809/dsafasdfasdf.png?sign=_QGe0dW7S17pNT5pPBQKAehOaIoBxn-pkOQ5epMQYAg%3D%3A0"></p>]]></content>
    
    
    <categories>
      
      <category>数学建模</category>
      
      <category>图论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学建模</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图论基础及绘制(python)</title>
    <link href="/posts/18665.html"/>
    <url>/posts/18665.html</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>数学建模 图论 Python</p>          </div><h2 id="图的基础理论">图的基础理论</h2><h3 id="图的基本概念">图的基本概念</h3><p>定义1：图是一些点和这些点之间的连线组成的。定义为<span class="math inline">\(G=(V,E)\)</span>,<span class="math inline">\(V\)</span>是顶点(vertex)的非空有限集合，称为顶点集,<span class="math inline">\(E\)</span>是边(edge)的集合，称为边集。边一般用<span class="math inline">\((v_i,v_j)\)</span>表示，其中<span class="math inline">\(v_i,v_j\in V\)</span>,用<span class="math inline">\(\vert V \vert\)</span>表示图中顶点的个数，<span class="math inline">\(\vert E\vert\)</span>表示边的条数。</p><h3 id="有向图和无向图">有向图和无向图</h3><p>定义2：图的边有方向称为有向图，没有方向称为弧。有向图的弧的起点称为弧头，弧的终点称为弧尾。有向图一般记为<span class="math inline">\(D=(V,A)\)</span>,其中<span class="math inline">\(V\)</span>为顶点集,<span class="math inline">\(A\)</span>为弧集。</p><h3 id="简单图和完全图">简单图和完全图</h3><p>定义3：设<span class="math inline">\(e=(u,v)\)</span>是图<span class="math inline">\(G\)</span>的一条边,则称<span class="math inline">\(u,v\)</span>是<span class="math inline">\(e\)</span>的端点，并称<span class="math inline">\(u,v\)</span>相邻，边<span class="math inline">\(e\)</span>与顶点<span class="math inline">\(u或(v)\)</span>相关联。若两条边<span class="math inline">\(e_i,e_j\)</span>有共同的端点，则称<span class="math inline">\(e_i,e_j\)</span>相邻，称有相同端点的两条边为重边。称两端点相同的边为环。称不与任何边相关联的顶点为孤立点。</p><p>定义4：无环且无重边的图称为简单图</p><p>定义5：任意两顶点均相邻的简单图称为完全图，含有<span class="math inline">\(n\)</span>个顶点的完全图记为<span class="math inline">\(K_n\)</span></p><h3 id="赋权图">赋权图</h3><p>定义6：如果图<span class="math inline">\(G\)</span>的每一条边<span class="math inline">\(e\)</span>都附有一个实数<span class="math inline">\(w(e)\)</span>，则称图<span class="math inline">\(G\)</span>为赋权图,实数<span class="math inline">\(w(e)\)</span>称为边<span class="math inline">\(e\)</span>的权</p><h3 id="顶点的度">顶点的度</h3><p>定义7：（1）在无向图中与顶点<span class="math inline">\(v\)</span>相关联的边(环算两次)的数目称为<span class="math inline">\(v\)</span>的度,记作<span class="math inline">\(d(v)\)</span></p><p>​ （2）在有向图中，从顶点<span class="math inline">\(v\)</span>引出的弧的数目称为<span class="math inline">\(v\)</span>的出度，记作<span class="math inline">\(d^{+}(v)\)</span>,从顶点<span class="math inline">\(v\)</span>引入弧的数目称为弧的入度，记作<span class="math inline">\(d^{-}(v)\)</span>， <span class="math inline">\(d(v)=d^{+}(v)+d^{-}(v)\)</span>称为<span class="math inline">\(v\)</span>的度</p><pre><code class="hljs">                度为奇数的顶点称为奇顶点，度为偶数的顶点称为偶顶点。</code></pre><p>定理1：给定图<span class="math inline">\(G=(V,E)\)</span>,所有顶点的度数之和是边数的2倍，即<span class="math display">\[\sum_{v\in V}d(v) = 2\vert E\vert\]</span> 推论：任何图中奇顶点的数目总为偶数</p><h3 id="子图">子图</h3><p>定义8：设<span class="math inline">\(G_1=(V_1,E_1)\)</span>v与<span class="math inline">\(G_2=(V_2,E_2)\)</span>是两个图,并且满足<span class="math inline">\(V_1\subset V_2,E_1\subset E_2\)</span>，则称<span class="math inline">\(G_1\)</span>是<span class="math inline">\(G_2\)</span>的子图。如果<span class="math inline">\(G_1\)</span>是<span class="math inline">\(G_2\)</span>的子图，且<span class="math inline">\(V_1=V_2\)</span>，则称<span class="math inline">\(G_1\)</span>是<span class="math inline">\(G_2\)</span>的生成子图。</p><h3 id="道路与回路">道路与回路</h3><p>设<span class="math inline">\(W=v_0e_1v_1e_2\cdotse_kv_k,\,where\quad e_i\in E,v_j\in V\)</span>，<span class="math inline">\(e_i\)</span>与<span class="math inline">\(v_{i-1}\)</span>和<span class="math inline">\(v_i\)</span>相关联，称<span class="math inline">\(W\)</span>是图<span class="math inline">\(G\)</span>的一条道路，简称路，<span class="math inline">\(k\)</span>为路长，<span class="math inline">\(v_0\)</span>为起点,<span class="math inline">\(v_k\)</span>为终点；各边相异的道路称为迹(trail);各顶点相异的道路称为轨道(path),记为<span class="math inline">\(P(v_0,v_k)\)</span>；起点和终点相同的道路称为回路；起点和终点重合的轨道称为圈。称以两顶点<span class="math inline">\(u,v\)</span>分别为起点和终点的最短轨道之长为顶点<span class="math inline">\(u,v\)</span>的距离。</p><h3 id="连通图和非连通图">连通图和非连通图</h3><p>在无向图<span class="math inline">\(G\)</span>中，如果从顶点<span class="math inline">\(u\)</span>到顶点<span class="math inline">\(v\)</span>存在道路，则称顶点<span class="math inline">\(u\)</span>和顶点<span class="math inline">\(v\)</span>是连通的。如果图<span class="math inline">\(G\)</span>中任意两个顶点都是连通的，则称图<span class="math inline">\(G\)</span>是联通图，否则称为非连通图。</p><h2 id="图的表示">图的表示</h2><p>以下均假设图<span class="math inline">\(G=(V,E)\)</span>为简单图，其中<span class="math inline">\(V=\{v_1,v_2,\cdots,v_n\},E=\{e_1,e_2,\cdots,e_m\}\)</span></p><ul><li>关联矩阵</li></ul><p>对于无向图<span class="math inline">\(G\)</span>，其关联矩阵<span class="math inline">\(M=(M_{ij})_{n\times m}\)</span>,其中 <span class="math display">\[m_{ij} = \begin{cases}1,\quad &amp;v_i\text{与}e_j\text{相关联}\\                    0，\quad &amp;v_i\text{与}e_j\text{不关联}          \end{cases}\]</span> 对有向图<span class="math inline">\(G\)</span>,其关联矩阵<span class="math inline">\(M=(M_{ij})_{n\times m}\)</span>，其中 <span class="math display">\[m_{ij} = \begin{cases}1,\quad &amp;v_i\text{是}e_j\text{起点}\\                    -1，\quad &amp;v_i\text{是}e_j\text{终点}\\                    0,\quad &amp;v_i\text{与}e_j\text{不关联}          \end{cases}\]</span></p><ul><li>邻接矩阵</li></ul><p>对于无向非赋权图<span class="math inline">\(G\)</span>，其邻接矩阵<span class="math inline">\(W=(w_{ij})_{n\times n}\)</span>其中 <span class="math display">\[w_{ij} = \begin{cases}1,\quad &amp;v_i\text{与}v_j\text{相邻}\\                    0，\quad &amp;v_i\text{与}v_j\text{不相邻}          \end{cases}\]</span> 对于有向非赋权图<span class="math inline">\(G\)</span>,其邻接矩阵<span class="math inline">\(W=(w_{ij})_{n\times n}\)</span>其中 <span class="math display">\[w_{ij} = \begin{cases}1,\quad &amp;(v_i,v_j)\in A\\                    0，\quad &amp;(v_i,v_j)\notin A          \end{cases}\]</span> 对于无向赋权图，其邻接矩阵<span class="math inline">\(W=(w_{ij})_{n\times n}\)</span>其中 <span class="math display">\[w_{ij} =\begin{cases}\text{顶点}v_i\text{顶点}v_j\text{之间边的权},\quad&amp;(v_i,v_j)\in E\\                    0(\text{或}\infty)，\quad&amp;v_i,v_j\text{之间无边}          \end{cases}\]</span></p><h2 id="绘制图">绘制图</h2><h3 id="绘制无向加权图">绘制无向加权图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#绘制无向加权图</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> networkx <span class="hljs-keyword">as</span> nx<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br>sns.set_theme(style=<span class="hljs-string">&quot;dark&quot;</span>)<span class="hljs-comment">#用来设置主题背景</span><br>plt.rcParams[<span class="hljs-string">&quot;font.sans-serif&quot;</span>] = [<span class="hljs-string">&quot;SimHei&quot;</span>] <span class="hljs-comment">#用来正常显示中文标签</span><br>plt.rcParams[<span class="hljs-string">&quot;axes.unicode_minus&quot;</span>] = <span class="hljs-literal">False</span> <span class="hljs-comment">#用来正常显示负号</span><br>A = np.array([[<span class="hljs-number">0</span>,<span class="hljs-number">9</span>,<span class="hljs-number">2</span>,<span class="hljs-number">4</span>,<span class="hljs-number">7</span>]<br>             ,[<span class="hljs-number">9</span>,<span class="hljs-number">0</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0</span>]<br>             ,[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">0</span>,<span class="hljs-number">8</span>,<span class="hljs-number">4</span>]<br>             ,[<span class="hljs-number">4</span>,<span class="hljs-number">4</span>,<span class="hljs-number">8</span>,<span class="hljs-number">0</span>,<span class="hljs-number">6</span>]<br>             ,[<span class="hljs-number">7</span>,<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">6</span>,<span class="hljs-number">0</span>]]) <span class="hljs-comment">#输入邻接矩阵</span><br>i,j = np.nonzero(A) <span class="hljs-comment">#提取顶点编号</span><br>w = A[i,j] <span class="hljs-comment">#提取A非零元素</span><br>edges = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">zip</span>(i,j,w)) <span class="hljs-comment">#构建边list</span><br>G = nx.Graph()<span class="hljs-comment">#创建无向图类</span><br>G.add_weighted_edges_from(edges) <span class="hljs-comment">#添加带权边</span><br>node_labels = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>),[<span class="hljs-built_in">str</span>(i+<span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>)]))<span class="hljs-comment">#顶点标签</span><br>pos = nx.shell_layout(G) <span class="hljs-comment">#图形布局</span><br>nx.draw_networkx(G,pos,node_size=<span class="hljs-number">260</span>,labels=node_labels) <span class="hljs-comment">#绘制图</span><br>w = nx.get_edge_attributes(G,<span class="hljs-string">&quot;weight&quot;</span>)<span class="hljs-comment">#获取边的权重字典</span><br>nx.draw_networkx_edge_labels(G,pos,font_size=<span class="hljs-number">12</span>,edge_labels=w)<span class="hljs-comment">#标记权重</span><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/06/26/IOyV2s3AqZ6pXrW.png"></p>]]></content>
    
    
    <categories>
      
      <category>数学建模</category>
      
      <category>图论</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>逻辑回归</title>
    <link href="/posts/60504.html"/>
    <url>/posts/60504.html</url>
    
    <content type="html"><![CDATA[<div class="note note-success">            <p>机器学习 逻辑回归</p>          </div><h2 id="广义线性模型generalized-linear-model">广义线性模型（Generalizedlinear model）</h2><p>设<span class="math inline">\(y=\hat{w}^{T}\cdot\hat{x},where\,\hat{w}=(w_1,w_2,\dots,w_n,b)^{T}\,\hat{x}=(x_1,x_2,\dots,x_n,1)^{T}\)</span>,令<span class="math inline">\(g(y)=\hat{w}^{T}\cdot\hat{x}\)</span>，其中<span class="math inline">\(y=g(x)\)</span>为连续可微函数。则该模型为广义线性模型，该模型的提出是为了解决线性模型由于简单而带有的局限性。若令<span class="math inline">\(y=g(x)=ln(x)\)</span>，则该模型可表示为<span class="math inline">\(ln(y)=\hat{w}^{T}\cdot\hat{x} \iffy=\exp(\hat{w}^{T}\cdot\hat{x})\)</span>，该模型被称为对数线性模型(logistlinear model).</p><h2 id="对数几率模型与逻辑回归">对数几率模型与逻辑回归</h2><ul><li>几率(odd)</li></ul><p>几率不是概率，而是一个事件发生与不发生的概率的比值。假设某事件A发生的概率为p，则该事件不发生的概率为1-p，该事件的几率为：<span class="math inline">\(odd(A)=\frac{p}{1-p}\)</span>.在几率的基础上取（自然底数的）对数，则构成该事件的对数几率（logit）：<span class="math inline">\(logit(A)=\ln\frac{p}{1-p}\)</span></p><ul><li>对数几率模型</li></ul><p>如果我们将对数几率看成是一个函数，并将其作为联系函数，即<span class="math inline">\(g(y)=\ln\frac{y}{1-y}\)</span>.则该广义线性模型为：<span class="math display">\[\ln\frac{y}{1-y}=\hat{w}^{T}\cdot\hat{x} \iffy=\frac{1}{1+\exp(-\hat{w}^{T}\cdot\hat{x})}\quad where\quady=\frac{1}{1+exp(-z)}被称为Sigmoid函数\]</span></p><ul><li>Sigmoid函数及其导数</li></ul><p><span class="math inline">\(Sigmoid(x)=\frac{1}{1+\exp(-x)},Sigmoid^{\prime}(x)=(1-Sigmoid(x))Sigmoid(x)\)</span>，其图像如下：</p><p><img src="https://s2.loli.net/2022/06/26/NOwe82Qgz5tlvPj.png"></p><h2 id="逻辑回归模型输出结果与模型可解释性">逻辑回归模型输出结果与模型可解释性</h2><ul><li>连续型输出结果转化为分类预测结果</li></ul><p>对于逻辑回归输出的(0,1)之间的连续型数值，我们只需要确定一个“阈值”，就可以将其转化为二分类的类别判别结果。通常来说，这个阈值是0.5，即以0.5为界，调整模型输出结果：</p><p><span class="math display">\[\begin{equation}y_{cla}=\left\{\begin{aligned}0, y&lt;0.5 \\1, y≥0.5\end{aligned}\right.\end{equation}\]</span></p><p>而有时候逻辑回归当<span class="math inline">\(f1\_score\)</span>等作为分类评价指标可以将阈值进行调整。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#借助sklearn中的Logisticregression来构建带有阈值移动的评估器</span><br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><span class="hljs-keyword">from</span> sklearn.base <span class="hljs-keyword">import</span> BaseEstimator,TransformerMixin<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">logistic_threshold</span>(BaseEstimator,TransformerMixin):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,penalty=<span class="hljs-string">&quot;l2&quot;</span>,C=<span class="hljs-number">1.0</span>,solver=<span class="hljs-string">&quot;lbfgs&quot;</span>,max_iter=<span class="hljs-built_in">int</span>(<span class="hljs-params"><span class="hljs-number">1e8</span></span>),l1_ratio=<span class="hljs-literal">None</span>,class_weight=<span class="hljs-literal">None</span>,thr=<span class="hljs-number">0.5</span></span>):<br>        self.penalty = penalty<br>        self.C = C<br>        self.solver = solver<br>        self.max_iter = max_iter<br>        self.l1_ratio = l1_ratio<br>        self.class_weight = class_weight<br>        self.thr = thr<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">fit</span>(<span class="hljs-params">self,X,y</span>):<br>        clf =      LogisticRegression(slef.penalty,self.C,self.solver,self.max_iter,self.l1_ratio,self.class_weight).fit(X,y)<br>        self.coef_ = clf.coef_<br>        self.clf = clf<br>        <span class="hljs-keyword">return</span> self<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self,X</span>):<br>        res = (self.clf.predict_proba(X)[:,<span class="hljs-number">1</span>]&gt;self.thr)*<span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> res<br></code></pre></td></tr></table></figure><ul><li>逻辑回归输出结果(y)是否是概率</li></ul><p>决定y是否是概率的核心因素，不是模型本身，而是建模流程。  逻辑斯蒂本身也有对应的概率分布，因此输入的自变量其实是可以视作随机变量的，但前提是需要满足一定的分布要求。如果逻辑回归的建模流程遵照数理统计方法的一般建模流程，即自变量的分布（或者转化之后的分布）满足一定要求（通过检验），则最终模型输出结果就是严格意义上的概率取值。而如果是遵照机器学习建模流程进行建模，在为对自变量进行假设检验下进行模型构建，则由于自变量分布不一定满足条件，因此输出结果不一定为严格意义上的概率。</p><p>而我们基本都采用机器学习建模流程进行逻辑回归的构建，因此对于模型输出结果y，其实并不一定是严格意义上的概率。不过在目前大多数使用场景中，由于大家希望能够用到模型本身的可解释性，因此还是会将模型结果解读为1发生的概率。尽管这并不是一个严谨的做法，但在机器学习整体的“实证”倾向下，只要业务方接受这种做法、并且能够一定程度指导业务，我们就可以将其解读为概率。我们将逻辑回归输出结果看成近似概率值。</p><h2 id="sklearn中的逻辑回归">sklearn中的逻辑回归</h2><ul><li>正则化(Regularization)</li></ul><p><img src="https://s2.loli.net/2022/06/26/oq5lQmuFOhEkDZP.png"></p><p>上述为sklearn官方解释可以看出sklearn中是默认进行正则化，每个式子的前面部分为结构风险项（正则化项）,后面部分为损失函数经验风险项。当我们不想进行正则化的时候可以控制C很大。</p><table><thead><tr class="header"><th>参数</th><th>解释</th></tr></thead><tbody><tr class="odd"><td>penalty</td><td>正则化项</td></tr><tr class="even"><td>dual</td><td>是否求解对偶问题*</td></tr><tr class="odd"><td>tol</td><td>迭代停止条件：两轮迭代损失值差值小于tol时，停止迭代</td></tr><tr class="even"><td>C</td><td>经验风险和结构风险在损失函数中的权重</td></tr><tr class="odd"><td>fit_intercept</td><td>线性方程中是否包含截距项</td></tr><tr class="even"><td>intercept_scaling</td><td>相当于此前讨论的特征最后一列全为1的列，当使用liblinear求解参数时用于捕获截距</td></tr><tr class="odd"><td>class_weight</td><td>各类样本权重*</td></tr><tr class="even"><td>random_state</td><td>随机数种子</td></tr><tr class="odd"><td>solver</td><td>损失函数求解方法*</td></tr><tr class="even"><td>max_iter</td><td>求解参数时最大迭代次数，迭代过程满足max_iter或tol其一即停止迭代</td></tr><tr class="odd"><td>multi_class</td><td>多分类问题时求解方法*</td></tr><tr class="even"><td>verbose</td><td>是否输出任务进程</td></tr><tr class="odd"><td>warm_start</td><td>是否使用上次训练结果作为本次运行初始参数</td></tr><tr class="even"><td>l1_ratio</td><td>当采用弹性网正则化时，<span class="math inline">\(l1\)</span>正则项权重，就是损失函数中的<span class="math inline">\(\rho\)</span></td></tr></tbody></table><ul><li>dual：是否求解对偶问题</li></ul><p>  对偶问题是约束条件相反、求解方向也相反的问题，当数据集过小而特征较多时，求解对偶问题能一定程度降低运算复杂度，其他情况建议保留默认参数取值。</p><ul><li>class_weight:各类样本权重</li></ul><p>  class_weight其实代表各类样本在进行损失函数计算时的数值权重，例如假设一个二分类问题，0、1两类的样本比例是2:1，此时可以输入一个字典类型对象用于说明两类样本在进行损失值计算时的权重，例如输入:{0:1,1:3}，则代表1类样本的每一条数据在进行损失函数值的计算时都会在原始数值上*3。而当我们将该参数选为<code>balanced</code>时，则会自动将这个比例调整为真实样本比例的反比，以达到平衡的效果。</p><ul><li>solver：损失函数求解方法</li></ul><p>  其实除了最小二乘法和梯度下降以外，还有非常多的关于损失函数的求解方法，而选择损失函数的参数，就是solver参数。</p><p>  而当前损失函数到底采用何种优化方法进行求解，其实最终目的是希望能够更快（计算效率更高）更好（准确性更高）的来进行求解，而硬性的约束条件是损失函数的形态，此外则是用户自行选择的空间。下面为官方给出的solver列表</p><p><img src="https://s2.loli.net/2022/06/26/gAFjBUWPho7C8nG.png"></p><ul><li>multi_class：选用何种方法进行多分类问题求解</li></ul><p>可选OVR和MVM，当然默认情况是auto，此时模型会优先根据惩罚项和solver选择OVR还是MVM，但一般来说，MVM效果会好于OVR。</p><h2 id="利用极大似然估计进行参数估计">利用极大似然估计进行参数估计</h2><p>逻辑回归模型如下： <span class="math display">\[y = \frac{1}{1+\exp(-\hat{w}^{T}\cdot\hat{x})}\quad\hat{w}^{T}=(w_1,w_2,\dots,w_n,b)^{T}\,,\hat{x}=(x_1,x_2,\dots,x_n,1)^{T}\\\]</span> 固定一组<span class="math inline">\(\hat{w}^{T}\)</span>和<span class="math inline">\(\hat{x}\)</span>得:</p><p><span class="math display">\[p(y=1\vert\hat{w}^{T},\hat{x}) =\frac{1}{1+\exp(-\hat{w}^{T}\cdot\hat{x})} \quadp(y=1\vert\hat{w}^{T},\hat{x}) =\frac{1}{1+\exp(-\hat{w}^{T}\cdot\hat{x})}\\\]</span></p><p>令<span class="math inline">\(p_1(\hat{w}^{T},\hat{x})=p(y=1\vert\hat{w}^{T},\hat{x}),p_0(\hat{w}^{T},\hat{x})=p(y=0\vert\hat{w}^{T},\hat{x})\)</span></p><p>得：<span class="math inline">\(p(y=y_i)=p_1^{y_i}(\hat{w}^{T},\hat{x})\cdotp_0^{1-y_i}(\hat{w}^{T},\hat{x})\)</span></p><p>则似然函数为<span class="math inline">\(L(y_i;\hat{w}^{T},\hat{x})=\prod_{i=1}^{n}p(y=y_i)=\prod_{i=1}^{n}p_1^{y_i}(\hat{w}^{T},\hat{x})\cdotp_0^{1-y_i}(\hat{w}^{T},\hat{x})\)</span></p><p>进而<span class="math inline">\(\ln L=\sum_{i=1}^{n}(\lnp_1^{y_i}(\hat{w}^{T},\hat{x})+\lnp_0^{1-y_i}(\hat{w}^{T},\hat{x}))=\sum_{i=1}^{n}(y_i\lnp_1(\hat{w}^{T},\hat{x})+(1-y_i)\lnp_0(\hat{w}^{T},\hat{x}))\)</span></p><p>通过一系列数学过程可以证明，通过极大似然估计构建的损失函数是凸函数，此时我们可以采用导数为0联立方程组的方式进行求解，这也是极大似然估计对参数求解的一般方法。但这种方法会涉及大量的导数运算、方程组求解等，并不适用于大规模甚至是超大规模数值运算，因此，在机器学习领域，我们通常会采用一些更加通用的优化方法对逻辑回归的损失函数进行求解，通常来说是牛顿法或者梯度下降算法，其中，梯度下降算法是机器学习中最为通用的求解损失函数的优化算法.</p><p>为了方便求最小值将其转化为: <span class="math display">\[\ln L=-\sum_{i=1}^{n}(y_i\ln p_1(\hat{w}^{T},\hat{x})+(1-y_i)\lnp_0(\hat{w}^{T},\hat{x}))\]</span></p><h2 id="熵相对熵与交叉熵">熵、相对熵与交叉熵</h2><p>###熵</p><p>通常我们用熵（entropy）来表示随机变量不确定性的度量，或者说系统混乱程度、信息混乱程度。熵的计算公式如下：<span class="math display">\[entrop(X) = -\sum_{i=1}^{n}p(x_i)log(x_i)\]</span>可以证明，熵的计算结果在[0,1]之间，并且熵值越大，系统越混乱、信息越混乱。</p><h3 id="相对熵和交叉熵">相对熵和交叉熵</h3><p>相对熵也被称为Kullback-Leibler散度（KL散度）或者信息散度（informationdivergence）。通常用来衡量两个随机变量分布的差异性。假设对同一个随机变量X，有两个单独的概率分布P(x)和Q(x)，当X是离散变量时，我们可以通过如下相对熵计算公式来衡量二者差异：<span class="math display">\[D_{KL}(P\vert\vert Q) = \sum_{i=1}^{n}P(x_i)log(\frac{P(x_i)}{Q(x_i)})\]</span> 和信息熵类似，相对熵越小，代表Q(x)和P(x)越接近。</p><p>从交叉熵的计算公式不难看出，这其实是一种非对称性度量，也就是<span class="math inline">\(D_{KL}(P\vert\vert Q)\ne D_{KL}(Q\vert\vertP)\)</span>。从本质上来说，相对熵刻画的是用概率分布Q来刻画概率分布P的困难程度，而在机器学习领域，我们一般令Q为模型输出结果，而P为数据集标签真实结果，以此来判断模型输出结果是否足够接近真实情况。</p><ul><li>Q为拟合分布P为真实分布，也被称为前向KL散度（forward KLdivergence）。</li></ul><p><span class="math display">\[\begin{split}D_{KL}(P\vert\vert Q) &amp;=\sum_{i=1}^{n}P(x_i)log(\frac{P(x_i)}{Q(x_i)})\\&amp;=\sum_{i=1}^{n}P(x_i)log(P(x_i))-\sum_{i=1}^{n}P(x_i)log(Q(x_i))\\&amp;=-entropy(P(X))+cross\_entropy(P,Q)\end{split}\]</span></p><p>对于给定数据集，其信息熵是确定的。因此，如果我们希望P、Q二者分布尽可能接近，我们就需要尽可能减少相对熵，但由于相对熵=交叉熵-信息熵，因此我们只能力求减少交叉熵。当然，也正因如此，交叉熵可以作为衡量模型输出分布是否接近真实分布的重要度量方法。</p><p>简单总结上述过程要点：</p><ul><li>我们用相对熵<span class="math inline">\(𝐷_{𝐾𝐿}(𝑃||𝑄)\,D_{KL}(P||Q)\)</span>来表示模型拟合分布Q和数据真实分布P之间的差距，相对熵越小拟合效果越好；</li><li>根据计算公式(6),相对熵=交叉熵-信息熵；</li><li>对于给定数据集，信息熵是确定的，因此我们只能通过尽可能减小交叉熵来降低相对熵；</li></ul><p>根据吉布斯不等式，相对熵的取值恒大于等于零，当预测分布和真实分布完全一致时相对熵取值为0，此时交叉熵等于数据信息熵，此外只要二者分布不一致，交叉熵的取值都将大于信息熵。</p><h3 id="二分类交叉熵损失函数">二分类交叉熵损失函数</h3><p><span class="math display">\[binaryCE(\hat{w}) =-\sum_{i=1}^{n}(y_ilog(p_1(\hat{w}^{T},\hat{x}))+(1-y_i)log(p_0(\hat{w}^{T},\hat{x})))\]</span></p><h2 id="逻辑回归应用">逻辑回归应用</h2><p>该数据集为Telco Customer Churn 电信用户流失预测案例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br>sns.set_theme(style=<span class="hljs-string">&quot;darkgrid&quot;</span>)<span class="hljs-comment">#用来设置主题背景</span><br>plt.rcParams[<span class="hljs-string">&quot;font.sans-serif&quot;</span>] = [<span class="hljs-string">&quot;SimHei&quot;</span>] <span class="hljs-comment">#用来正常显示中文标签</span><br>plt.rcParams[<span class="hljs-string">&quot;axes.unicode_minus&quot;</span>] = <span class="hljs-literal">False</span> <span class="hljs-comment">#用来正常显示负号</span><br><span class="hljs-comment"># Read data</span><br>tcc = pd.read_csv(<span class="hljs-string">&#x27;WA_Fn-UseC_-Telco-Customer-Churn.csv&#x27;</span>)<br><span class="hljs-comment"># 标注连续/离散字段</span><br><span class="hljs-comment"># 离散字段</span><br>category_cols = [<span class="hljs-string">&#x27;gender&#x27;</span>, <span class="hljs-string">&#x27;SeniorCitizen&#x27;</span>, <span class="hljs-string">&#x27;Partner&#x27;</span>, <span class="hljs-string">&#x27;Dependents&#x27;</span>,<br>               <span class="hljs-string">&#x27;PhoneService&#x27;</span>, <span class="hljs-string">&#x27;MultipleLines&#x27;</span>, <span class="hljs-string">&#x27;InternetService&#x27;</span>, <span class="hljs-string">&#x27;OnlineSecurity&#x27;</span>, <span class="hljs-string">&#x27;OnlineBackup&#x27;</span>, <br>                <span class="hljs-string">&#x27;DeviceProtection&#x27;</span>, <span class="hljs-string">&#x27;TechSupport&#x27;</span>, <span class="hljs-string">&#x27;StreamingTV&#x27;</span>, <span class="hljs-string">&#x27;StreamingMovies&#x27;</span>, <span class="hljs-string">&#x27;Contract&#x27;</span>, <span class="hljs-string">&#x27;PaperlessBilling&#x27;</span>,<br>                <span class="hljs-string">&#x27;PaymentMethod&#x27;</span>]<br><br><span class="hljs-comment"># 连续字段</span><br>numeric_cols = [<span class="hljs-string">&#x27;tenure&#x27;</span>, <span class="hljs-string">&#x27;MonthlyCharges&#x27;</span>, <span class="hljs-string">&#x27;TotalCharges&#x27;</span>]<br><br><span class="hljs-comment"># 标签</span><br>target = <span class="hljs-string">&#x27;Churn&#x27;</span><br><br><span class="hljs-comment"># ID列</span><br>ID_col = <span class="hljs-string">&#x27;customerID&#x27;</span><br><br><span class="hljs-comment"># 验证是否划分能完全</span><br><span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(category_cols) + <span class="hljs-built_in">len</span>(numeric_cols) + <span class="hljs-number">2</span> == tcc.shape[<span class="hljs-number">1</span>]<br><span class="hljs-comment">#Verify whether the ID column is duplicate</span><br>tcc[ID_col].nunique() == tcc.shape[<span class="hljs-number">0</span>]<br><span class="hljs-comment">#Verify whether have the explict missing values</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">missing</span>(<span class="hljs-params">df</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    Caculate the proportion of the missing value</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    missing_number = df.isnull().<span class="hljs-built_in">sum</span>().sort_values(ascending=<span class="hljs-literal">False</span>) <span class="hljs-comment">#decending</span><br>    missing_per = (df.isnull().<span class="hljs-built_in">sum</span>()/df.count()).sort_values(ascending=<span class="hljs-literal">False</span>) <span class="hljs-comment">#decending</span><br>    missing_df = pd.concat([missing_number,missing_per],axis=<span class="hljs-number">1</span>,keys=[<span class="hljs-string">&quot;Missing_number&quot;</span>,<span class="hljs-string">&quot;Missing_per&quot;</span>])<br>    <span class="hljs-keyword">return</span> missing_df<br><span class="hljs-comment">#Verify whether have the discrete explict missing values</span><br><span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> category_cols:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&#123;&#125;:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(feature,tcc[feature].unique()))<br> <span class="hljs-comment">#Verify whether have the continuous explict missing values</span><br>tcc[numeric_cols].astype(<span class="hljs-built_in">float</span>)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">find_index</span>(<span class="hljs-params">data_col,value</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Verify the first occurance of the given value in the given column,return -1 if there is none.</span><br><span class="hljs-string">    param data_col:given column</span><br><span class="hljs-string">    param value:given value</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    val_lst = [value]<br>    <span class="hljs-keyword">if</span> data_col.isin(val_lst).<span class="hljs-built_in">sum</span>() == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">return</span> -<span class="hljs-number">1</span><br>    <span class="hljs-keyword">else</span>:<br>        res = data_col.isin(val_lst).idxmax()<br>    <span class="hljs-keyword">return</span> res<br><span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> numeric_cols:<br>    <span class="hljs-built_in">print</span>(find_index(tcc[feature],<span class="hljs-string">&quot; &quot;</span>))<br>tcc[<span class="hljs-string">&quot;TotalCharges&quot;</span>] = tcc[<span class="hljs-string">&quot;TotalCharges&quot;</span>].apply(<span class="hljs-keyword">lambda</span> x:x <span class="hljs-keyword">if</span> x!=<span class="hljs-string">&quot; &quot;</span><span class="hljs-keyword">else</span> np.nan).astype(<span class="hljs-built_in">float</span>)<br>tcc[<span class="hljs-string">&quot;MonthlyCharges&quot;</span>] = tcc[<span class="hljs-string">&quot;MonthlyCharges&quot;</span>].astype(<span class="hljs-built_in">float</span>)<br>tcc[<span class="hljs-string">&quot;TotalCharges&quot;</span>]=tcc[<span class="hljs-string">&quot;TotalCharges&quot;</span>].fillna(<span class="hljs-number">0</span>)<br><span class="hljs-comment">#Outlier detection</span><br>plt.figure(figsize=(<span class="hljs-number">16</span>, <span class="hljs-number">6</span>), dpi=<span class="hljs-number">200</span>)<br>plt.subplot(<span class="hljs-number">121</span>)<br>plt.boxplot(tcc[<span class="hljs-string">&#x27;MonthlyCharges&#x27;</span>])<br>plt.xlabel(<span class="hljs-string">&#x27;MonthlyCharges&#x27;</span>)<br>plt.subplot(<span class="hljs-number">122</span>)<br>plt.boxplot(tcc[<span class="hljs-string">&#x27;TotalCharges&#x27;</span>])<br>plt.xlabel(<span class="hljs-string">&#x27;TotalCharges&#x27;</span>)<br>tcc[target].replace(to_replace=<span class="hljs-string">&quot;Yes&quot;</span>,value=<span class="hljs-number">1</span>,inplace=<span class="hljs-literal">True</span>)<br>tcc[target].replace(to_replace=<span class="hljs-string">&quot;No&quot;</span>,value=<span class="hljs-number">0</span>,inplace=<span class="hljs-literal">True</span>)<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br>train,test = train_test_split(tcc,random_state=<span class="hljs-number">666</span>,test_size=<span class="hljs-number">0.3</span>)<br>X_train = train.drop(columns=[ID_col,target]).copy()<br>y_train = train[target].copy()<br>X_test = test.drop(columns=[ID_col,target]).copy()<br>y_test = test[target].copy()<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> OneHotEncoder,KBinsDiscretizer,StandardScaler<br><span class="hljs-keyword">from</span> sklearn.compose <span class="hljs-keyword">import</span> ColumnTransformer<br><span class="hljs-keyword">from</span> sklearn.pipeline <span class="hljs-keyword">import</span> make_pipeline<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> GridSearchCV<br>logistic_pre = ColumnTransformer([(<span class="hljs-string">&quot;cat&quot;</span>,OneHotEncoder(drop=<span class="hljs-string">&quot;if_binary&quot;</span>),category_cols),(<span class="hljs-string">&quot;num&quot;</span>,<span class="hljs-string">&quot;passthrough&quot;</span>,numeric_cols)])<br>logistic_model = LogisticRegression(max_iter=<span class="hljs-built_in">int</span>(<span class="hljs-number">1e8</span>))<br>logistic_pipe = make_pipeline(logistic_pre,logistic_model)<br>num_sel = [<span class="hljs-string">&quot;passthrough&quot;</span>,StandardScaler(),KBinsDiscretizer(n_bins=<span class="hljs-number">3</span>,encode=<span class="hljs-string">&quot;ordinal&quot;</span>,strategy=<span class="hljs-string">&quot;kmeans&quot;</span>)]<br>logistic_param = [&#123;<span class="hljs-string">&quot;columntransformer__num&quot;</span>:num_sel<br>                   ,<span class="hljs-string">&quot;logisticregression__penalty&quot;</span>:[<span class="hljs-string">&quot;l1&quot;</span>]<br>                   ,<span class="hljs-string">&quot;logisticregression__C&quot;</span>:np.arange(<span class="hljs-number">0.1</span>,<span class="hljs-number">2.1</span>,<span class="hljs-number">0.1</span>).tolist()<br>                   ,<span class="hljs-string">&quot;logisticregression__solver&quot;</span>:[<span class="hljs-string">&quot;saga&quot;</span>]&#125;<br>                 ,&#123;<span class="hljs-string">&quot;columntransformer__num&quot;</span>:num_sel<br>                   ,<span class="hljs-string">&quot;logisticregression__penalty&quot;</span>:[<span class="hljs-string">&quot;l2&quot;</span>]<br>                   ,<span class="hljs-string">&quot;logisticregression__C&quot;</span>:np.arange(<span class="hljs-number">0.1</span>,<span class="hljs-number">2.1</span>,<span class="hljs-number">0.1</span>).tolist()<br>                   ,<span class="hljs-string">&quot;logisticregression__solver&quot;</span>:[<span class="hljs-string">&#x27;lbfgs&#x27;</span>, <span class="hljs-string">&#x27;newton-cg&#x27;</span>, <span class="hljs-string">&#x27;sag&#x27;</span>, <span class="hljs-string">&#x27;saga&#x27;</span>]&#125;]<br>logistic_search = GridSearchCV(estimator=logistic_pipe,param_grid=logistic_param,n_jobs=-<span class="hljs-number">1</span>)<br><span class="hljs-keyword">import</span> time<br>s = time.time()<br>logistic_search.fit(X_train,y_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(time.time()-s))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">logit_threshold</span>(BaseEstimator, TransformerMixin):<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, penalty=<span class="hljs-string">&#x27;l2&#x27;</span>, C=<span class="hljs-number">1.0</span>, max_iter=<span class="hljs-number">1e8</span>, solver=<span class="hljs-string">&#x27;lbfgs&#x27;</span>, l1_ratio=<span class="hljs-literal">None</span>, class_weight=<span class="hljs-literal">None</span>, thr=<span class="hljs-number">0.5</span></span>):<br>        self.penalty = penalty<br>        self.C = C<br>        self.max_iter = max_iter<br>        self.solver = solver<br>        self.l1_ratio = l1_ratio<br>        self.thr = thr<br>        self.class_weight = class_weight<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">fit</span>(<span class="hljs-params">self, X, y</span>):<br>        clf = LogisticRegression(penalty = self.penalty, <br>                                 C = self.C, <br>                                 solver = self.solver, <br>                                 l1_ratio = self.l1_ratio,<br>                                 class_weight=self.class_weight, <br>                                 max_iter=self.max_iter)<br>        clf.fit(X, y)<br>        self.coef_ = clf.coef_<br>        self.clf = clf<br>        <span class="hljs-keyword">return</span> self<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, X</span>):<br>        res = (self.clf.predict_proba(X)[:, <span class="hljs-number">1</span>]&gt;=self.thr) * <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> res<br> <span class="hljs-comment"># 设置转化器流</span><br>logistic_pre = ColumnTransformer([<br>    (<span class="hljs-string">&#x27;cat&#x27;</span>, preprocessing.OneHotEncoder(drop=<span class="hljs-string">&#x27;if_binary&#x27;</span>), category_cols), <br>    (<span class="hljs-string">&#x27;num&#x27;</span>, <span class="hljs-string">&#x27;passthrough&#x27;</span>, numeric_cols)<br>])<br><br>num_pre = [<span class="hljs-string">&#x27;passthrough&#x27;</span>, preprocessing.StandardScaler(), preprocessing.KBinsDiscretizer(n_bins=<span class="hljs-number">3</span>, encode=<span class="hljs-string">&#x27;ordinal&#x27;</span>, strategy=<span class="hljs-string">&#x27;kmeans&#x27;</span>)]<br><br><span class="hljs-comment"># 实例化逻辑回归评估器</span><br>logistic_model = logit_threshold(max_iter=<span class="hljs-built_in">int</span>(<span class="hljs-number">1e8</span>))<br><br><span class="hljs-comment"># 设置机器学习流</span><br>logistic_pipe = make_pipeline(logistic_pre, logistic_model)<br><br><span class="hljs-comment"># 设置超参数空间</span><br>logistic_param = [<br>    &#123;<span class="hljs-string">&#x27;columntransformer__num&#x27;</span>:num_pre, <span class="hljs-string">&#x27;logit_threshold__thr&#x27;</span>: np.arange(<span class="hljs-number">0.1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0.1</span>).tolist(), <span class="hljs-string">&#x27;logit_threshold__penalty&#x27;</span>: [<span class="hljs-string">&#x27;l1&#x27;</span>], <span class="hljs-string">&#x27;logit_threshold__C&#x27;</span>: np.arange(<span class="hljs-number">0.1</span>, <span class="hljs-number">1.1</span>, <span class="hljs-number">0.1</span>).tolist(), <span class="hljs-string">&#x27;logit_threshold__solver&#x27;</span>: [<span class="hljs-string">&#x27;saga&#x27;</span>]&#125;, <br>    &#123;<span class="hljs-string">&#x27;columntransformer__num&#x27;</span>:num_pre, <span class="hljs-string">&#x27;logit_threshold__thr&#x27;</span>: np.arange(<span class="hljs-number">0.1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0.1</span>).tolist(), <span class="hljs-string">&#x27;logit_threshold__penalty&#x27;</span>: [<span class="hljs-string">&#x27;l2&#x27;</span>], <span class="hljs-string">&#x27;logit_threshold__C&#x27;</span>: np.arange(<span class="hljs-number">0.1</span>, <span class="hljs-number">1.1</span>, <span class="hljs-number">0.1</span>).tolist(), <span class="hljs-string">&#x27;logit_threshold__solver&#x27;</span>: [<span class="hljs-string">&#x27;lbfgs&#x27;</span>, <span class="hljs-string">&#x27;newton-cg&#x27;</span>, <span class="hljs-string">&#x27;sag&#x27;</span>, <span class="hljs-string">&#x27;saga&#x27;</span>]&#125;, <br>]<br><br><span class="hljs-comment"># 实例化网格搜索评估器</span><br>logistic_search_f1 = GridSearchCV(estimator = logistic_pipe,<br>                                  param_grid = logistic_param,<br>                                  scoring=<span class="hljs-string">&#x27;f1&#x27;</span>,<br>                                  n_jobs = <span class="hljs-number">12</span>)<br><br>s = time.time()<br>logistic_search_f1.fit(X_train, y_train)<br><span class="hljs-built_in">print</span>(time.time()-s, <span class="hljs-string">&quot;s&quot;</span>)<br></code></pre></td></tr></table></figure><h2 id="逻辑回归模型解释">逻辑回归模型解释</h2><p>对于逻辑回归的模型解释，核心是需要观察线性方程中自变量的系数，通过系数大小可以判断特征重要性，并且系数的具体数值也能表示因变量如何伴随自变量变化而变化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#查看逻辑回归系数</span><br>coe = logistic_search.best_estimator_.named_steps[<span class="hljs-string">&#x27;logit_threshold&#x27;</span>].coef_<br>coe = coe.flatten()<br>coe<br><span class="hljs-comment"># 定位独热编码转化器</span><br>tf = logistic_search.best_estimator_.named_steps[<span class="hljs-string">&#x27;columntransformer&#x27;</span>].named_transformers_[<span class="hljs-string">&#x27;cat&#x27;</span>]<br>tf<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">cate_colName</span>(<span class="hljs-params">Transformer, category_cols, drop=<span class="hljs-string">&#x27;if_binary&#x27;</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    离散字段独热编码后字段名创建函数</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    :param Transformer: 独热编码转化器</span><br><span class="hljs-string">    :param category_cols: 输入转化器的离散变量</span><br><span class="hljs-string">    :param drop: 独热编码转化器的drop参数</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <br>    cate_cols_new = []<br>    col_value = Transformer.categories_<br>    <br>    <span class="hljs-keyword">for</span> i, j <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(category_cols):<br>        <span class="hljs-keyword">if</span> (drop == <span class="hljs-string">&#x27;if_binary&#x27;</span>) &amp; (<span class="hljs-built_in">len</span>(col_value[i]) == <span class="hljs-number">2</span>):<br>            cate_cols_new.append(j)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> col_value[i]:<br>                feature_name = j + <span class="hljs-string">&#x27;_&#x27;</span> + f<br>                cate_cols_new.append(feature_name)<br>    <span class="hljs-keyword">return</span>(cate_cols_new)<br><span class="hljs-comment"># 转化后离散变量列名称</span><br>category_cols_new = cate_colName(tf, category_cols)<br><br><span class="hljs-comment"># 所有字段名称</span><br>cols_new = category_cols_new + numeric_cols<br><br><span class="hljs-comment"># 查看特征名称数量和特征系数数量是否一致</span><br><span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(cols_new) == <span class="hljs-built_in">len</span>(coe)<br></code></pre></td></tr></table></figure><p>对于<span class="math inline">\(\ln\frac{y}{1-y}=\hat{w}^{T}\cdot\hat{x}=w_1x_1+w_2x_2+\dots+w_nx_n+b\)</span>而言每个<span class="math inline">\(x_i\)</span>的变化会不同程度的影响对数几率的变化。</p><p>以<span class="math inline">\(\ln\frac{y}{1-y}=2x_1-x_2\)</span>为例<span class="math inline">\(x_1\)</span>每增长1,<span class="math inline">\(y\)</span>判别为正例的对数几率就增加2，概率就增加0.4</p><p>此外，还可以根据系数来判别特征重要性。</p>]]></content>
    
    
    <categories>
      
      <category>machine learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
